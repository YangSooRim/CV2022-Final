{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import os\n",
    "import io\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import imageio\n",
    "import skimage.io\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import color\n",
    "\n",
    "import faiss\n",
    "import json\n",
    "\n",
    "from NetVLAD.dataset import get_whole_val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Utility-guided Local Feature Matching')\n",
    "parser.add_argument(\n",
    "    '--input_dir', type=str, default='./data',\n",
    "    help='Path to the directory that contains the dataset')\n",
    "parser.add_argument('--dataset', type=str, default='berlin', \n",
    "    help='Dataset to use', choices=['oxford', 'nordland', 'berlin'])\n",
    "parser.add_argument(\n",
    "    '--output_dir', type=str, default='',\n",
    "    help='Path to the output directory to which the results and optionally, the visualizations are saved')\n",
    "parser.add_argument(\n",
    "    '--viz', action='store_true',\n",
    "    help='Visualize the best matches along with utility and dump as gif')\n",
    "parser.add_argument(\n",
    "    '--netvlad_extracts_path', type=str, default='',\n",
    "    help='Path to NetVLAD Extractions')\n",
    "parser.add_argument(\n",
    "    '--superpoint_extracts_path', type=str, default='',\n",
    "    help='Path to SuperPoint Extractions')\n",
    "parser.add_argument(\n",
    "    '--utility_path', type=str, default='',\n",
    "    help='Path to Folder containing PS Utility and Low ES Utility Clusters')\n",
    "parser.add_argument(\n",
    "    '--k', type=int, default=10,\n",
    "    help='Number of Top Utility Clusters')\n",
    "parser.add_argument(\n",
    "    '--es_utility', action='store_true',\n",
    "    help='Use Environment-Specific Utility')\n",
    "parser.add_argument(\n",
    "    '--ps_utility', action='store_true',\n",
    "    help='Use Place-Specific Utility')\n",
    "parser.add_argument(\n",
    "    '--non_default_k', action='store_true',\n",
    "    help='Use Non Default Number of Top Utility Clusters for Combined ES and PS Utility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig2img(fig):\n",
    "    \"\"\"Convert a Matplotlib figure to a PIL Image and return it\"\"\"\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf)\n",
    "    buf.seek(0)\n",
    "    img = Image.open(buf)\n",
    "\n",
    "    return img\n",
    "\n",
    "def match_descriptors(kp1, desc1, kp2, desc2):\n",
    "    # Match the keypoints with the warped_keypoints with nearest neighbor search\n",
    "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "    matches = bf.match(desc1, desc2)\n",
    "    matches_idx = np.array([m.queryIdx for m in matches])\n",
    "    m_kp1 = [kp1[idx] for idx in matches_idx]\n",
    "    matches_idx = np.array([m.trainIdx for m in matches])\n",
    "    m_kp2 = [kp2[idx] for idx in matches_idx]\n",
    "\n",
    "    return m_kp1, m_kp2, matches\n",
    "\n",
    "\n",
    "def compute_homography(matched_kp1, matched_kp2):\n",
    "    matched_pts1 = cv2.KeyPoint_convert(matched_kp1)\n",
    "    matched_pts2 = cv2.KeyPoint_convert(matched_kp2)\n",
    "\n",
    "    # Estimate the homography between the matches using RANSAC\n",
    "    H, inliers = cv2.findHomography(matched_pts1[:, [1, 0]],\n",
    "                                    matched_pts2[:, [1, 0]],\n",
    "                                    cv2.RANSAC)\n",
    "    inliers = inliers.flatten()\n",
    "\n",
    "    return H, inliers\n",
    "\n",
    "def drawMatches(imageA, imageB, kpsA, kpsB, matches):\n",
    "\t# initialize the output visualization image\n",
    "\t(hA, wA) = imageA.shape[:2]\n",
    "\t(hB, wB) = imageB.shape[:2]\n",
    "\tvis = np.zeros((max(hA, hB), wA + wB, 3), dtype=\"uint8\")\n",
    "\tvis[0:hA, 0:wA] = imageA\n",
    "\tvis[0:hB, wA:] = imageB[:,:,:3]\n",
    "\n",
    "\tfor i in range(len(kpsA)):\n",
    "\t\tptA = (int(kpsA[i][1]), int(kpsA[i][0]))\n",
    "\t\tcv2.circle(vis, ptA, radius=3, color=(235, 204, 255), thickness=1)\n",
    "\tfor i in range(len(kpsB)):\n",
    "\t\tptB = (int(kpsB[i][1]) + wA, int(kpsB[i][0]))\n",
    "\t\tcv2.circle(vis, ptB, radius=3, color=(235, 204, 255), thickness=1)\n",
    "\n",
    "\tqueryIdx = np.array([m.queryIdx for m in matches])\n",
    "\ttrainIdx = np.array([m.trainIdx for m in matches])\n",
    "\t# loop over the matches\n",
    "\tfor i in range(queryIdx.shape[0]):\n",
    "\t\t# draw the match\n",
    "\t\tptA = (int(kpsA[queryIdx[i]][1]), int(kpsA[queryIdx[i]][0]))\n",
    "\t\tptB = (int(kpsB[trainIdx[i]][1]) + wA, int(kpsB[trainIdx[i]][0]))\n",
    "\n",
    "\t\tcv2.circle(vis, ptA, radius=3, color=(255, 0, 0), thickness=1)\n",
    "\t\tcv2.circle(vis, ptB, radius=3, color=(255, 0, 0), thickness=1)\n",
    "\t\tcv2.line(vis, ptA, ptB, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "\treturn vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    opt = parser.parse_args()\n",
    "    print(opt)\n",
    "\n",
    "    # Create the output directories if they do not exist already.\n",
    "    output_dir = Path(opt.output_dir)\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # Loading cluster masks & netvlad candidates\n",
    "    db_cluster_masks = np.load(os.path.join(opt.netvlad_extracts_path, 'db_cluster_masks.npy'))\n",
    "    netvlad_preds = np.load(os.path.join(opt.netvlad_extracts_path, 'netvlad_preds.npy'))\n",
    "\n",
    "    # Load Reference Map Utilities\n",
    "    ps_utility = np.load(os.path.join(opt.utility_path, 'ps_utility.npy'))\n",
    "    ps_utility_rankings = np.flip(np.argsort(ps_utility, axis=1), axis=1)\n",
    "    \n",
    "    low_es_utility_clusters = np.load(os.path.join(opt.utility_path, 'low_es_utility_clusters.npy'))\n",
    "\n",
    "    idx = np.in1d(ps_utility_rankings, low_es_utility_clusters)\n",
    "    idx = ~idx\n",
    "    combined_utility_rankings = ps_utility_rankings.flatten()[idx].reshape((ps_utility.shape[0], \n",
    "                                            (ps_utility.shape[1] - low_es_utility_clusters.shape[0])))\n",
    "\n",
    "    if not opt.es_utility and not opt.ps_utility:\n",
    "        topk = ps_utility_rankings                                                       # All Clusters\n",
    "    elif opt.es_utility and not opt.ps_utility:\n",
    "        topk = combined_utility_rankings                                                 # High ES Utility Clusters (X)\n",
    "    elif opt.ps_utility and not opt.es_utility:\n",
    "        topk = ps_utility_rankings[:, :(opt.k)]                                          # Top K PS Utility Clusters\n",
    "    elif opt.ps_utility and opt.es_utility:\n",
    "        if opt.non_default_k:\n",
    "            topk = combined_utility_rankings[:, :(opt.k)]                                # Top K ES & PS Utility Clusters\n",
    "        else:\n",
    "            topk = combined_utility_rankings[:, :(combined_utility_rankings.shape[1]-1)] # Top X-1 ES & PS Utility Clusters\n",
    "\n",
    "    # Load SuperPoint Extractions\n",
    "    SP_all_db = np.load(os.path.join(opt.superpoint_extracts_path, 'db.npz'), allow_pickle=True)['arr_0']\n",
    "    SP_all_query = np.load(os.path.join(opt.superpoint_extracts_path, 'query.npz'), allow_pickle=True)['arr_0']\n",
    "\n",
    "    # Local Feature Matching\n",
    "    LFM_matches = []\n",
    "    match_scores = np.zeros((netvlad_preds.shape[0], netvlad_preds.shape[1]))\n",
    "\n",
    "    for i in tqdm(range(0,match_scores.shape[0]), desc='LFM'):\n",
    "\n",
    "        SP_q = SP_all_query[i]\n",
    "\n",
    "        for j in range(0,match_scores.shape[1]):\n",
    "\n",
    "            SP_db = SP_all_db[netvlad_preds[i, j].astype('int')]\n",
    "            db_cluster_mask = db_cluster_masks[netvlad_preds[i, j].astype('int')]\n",
    "            db_topk = topk[netvlad_preds[i, j].astype('int')]\n",
    "\n",
    "            kp_q = SP_q['keypoints'].tolist()\n",
    "            desc1 = SP_q['descriptors'].T\n",
    "\n",
    "            db_keypoints = np.flip(SP_db['keypoints'], axis=1)\n",
    "            db_map = db_cluster_mask[db_keypoints.astype('int')[:, 0].T, db_keypoints.astype('int')[:, 1].T]\n",
    "            db_filter = np.in1d(db_map, db_topk)\n",
    "            db_filter_ind = np.where(db_filter == 1)[0]\n",
    "\n",
    "            kp_db = SP_db['keypoints'][db_filter_ind, :].tolist()\n",
    "            desc2 = SP_db['descriptors'][:, db_filter_ind].T\n",
    "\n",
    "            kp1 = [cv2.KeyPoint(p[1], p[0], 1) for p in kp_q]\n",
    "\n",
    "            kp2 = [cv2.KeyPoint(p[1], p[0], 1) for p in kp_db]\n",
    "\n",
    "            if len(kp1) != 0 and len(kp2) != 0:\n",
    "                # Match and get rid of outliers\n",
    "                m_kp1, m_kp2, matches = match_descriptors(kp1, desc1, kp2, desc2)\n",
    "                H, inliers = compute_homography(m_kp1, m_kp2)\n",
    "\n",
    "                # Draw SuperPoint matches\n",
    "                matches = np.array(matches)[inliers.astype(bool)].tolist()\n",
    "\n",
    "                out_matches = {'keypoints0': np.flip(SP_q['keypoints'], axis=1).tolist(),\n",
    "                           'keypoints1': np.flip(SP_db['keypoints'][db_filter_ind, :], axis=1).tolist(),\n",
    "                           'matches': matches}\n",
    "\n",
    "                n_inlier = len(matches)\n",
    "                match_scores[i, j] = n_inlier/(len(kp_q) + len(kp_db))\n",
    "            else:\n",
    "                out_matches = {'keypoints0': np.flip(SP_q['keypoints'], axis=1).tolist(), \n",
    "                            'keypoints1': np.flip(SP_db['keypoints'][db_filter_ind, :], axis=1).tolist()}\n",
    "                match_scores[i, j] = 0\n",
    "\n",
    "            LFM_matches.append(out_matches)\n",
    "\n",
    "    dataset = get_whole_val_set(opt.input_dir, opt.dataset.lower())\n",
    "\n",
    "    knn = NearestNeighbors(n_jobs=1)\n",
    "    knn.fit(dataset.dbStruct.locDb)\n",
    "    _ , gt = knn.radius_neighbors(dataset.dbStruct.locQ,\n",
    "        radius=dataset.dbStruct.posDistThr)\n",
    "\n",
    "    predictions = np.zeros((netvlad_preds.shape[0], netvlad_preds.shape[1]))\n",
    "    best_match_ind = []\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        predictions[i,:] = netvlad_preds[i, np.flip(np.argsort(match_scores[i,:]))]\n",
    "        best_match_ind.append(np.flip(np.argsort(match_scores[i,:]))[0])\n",
    "\n",
    "    print('====> Calculating recall @ N')\n",
    "    n_values = [1,5,10,20]\n",
    "\n",
    "    file_path = \"./submit.json\"\n",
    "\n",
    "    data = {}\n",
    "    data['Query'] = list()\n",
    "\n",
    "    for i in range(len(predictions)) :\n",
    "        data_t = [(\"id\",i),(\"positive\",predictions[i].tolist())]\n",
    "        data_t = dict(data_t)\n",
    "        data['Query'].append(data_t)\n",
    "  \n",
    "    with open(file_path, 'w') as outfile:\n",
    "        json.dump(data, outfile, indent=4)\n",
    "\n",
    "    correct_at_n = np.zeros(len(n_values))\n",
    "\n",
    "    for qIx, pred in enumerate(predictions):\n",
    "        for i,n in enumerate(n_values):\n",
    "            # if in top N then also in top NN, where NN > N\n",
    "            if np.any(np.in1d(pred[:n], gt[qIx])):\n",
    "                correct_at_n[i:] += 1\n",
    "                break\n",
    "\n",
    "    recall_at_n = correct_at_n / dataset.dbStruct.numQ\n",
    "\n",
    "    for i,n in enumerate(n_values):\n",
    "        print(\"====> Recall@{}: {:.4f}\".format(n, recall_at_n[i]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('patchnetvlad')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1175b35a4b553c7187b8795f10b6f2fb25aab7cfb404163e2ad1c46c1314d8a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
